# Study Notes: Hashing in Data Structures

## 1. Introduction to Hashing
**Hashing** is a technique that converts a given key (e.g., a name, ID, or string) into an index within an array, called a **hash table**. The goal is to provide fast data insertion, deletion, and retrievalâ€”ideally in constant time $O(1)$.

### Why Hashing?
Traditional data structures have limitations:
- **Arrays:** $O(N)$ for search (unsorted), $O(\log N)$ for search (sorted), $O(N)$ for insertion/deletion.
- **Linked Lists:** $O(N)$ for search, insertion, deletion.
- **Balanced Binary Search Trees (e.g., AVL, Red-Black Trees):** $O(\log N)$ for search, insertion, deletion.

Hashing aims for $O(1)$ average-case time complexity for these operations, making it powerful for large datasets.

### Key Terminology
- **Key:** The input value to be stored or retrieved.
- **Hash Function:** A function that takes a key and returns an integer index (hash value).
- **Hash Value (Hash Code):** The integer index generated by the hash function.
- **Hash Table:** An array-like structure that stores data at indices computed by the hash function.
- **Collision:** When two different keys map to the same hash value (index). This is the central challenge in hashing.
- **Slot/Bucket:** An entry in the hash table where data is stored.

---

## 2. The Hash Function
The heart of any hashing system is the hash function. A good hash function is crucial for efficiency.

### 2.1. Properties of a Good Hash Function
1. **Deterministic:** Same input key always produces the same hash value.
2. **Efficiently Computable:** Fast to calculate.
3. **Uniform Distribution:** Distributes keys evenly across the table to minimize collisions.
4. **Low Collision Rate:** Collisions are inevitable, but should be minimized.

### 2.2. Common Hash Function Techniques
Let $k$ be the key, $m$ the table size, and $h(k)$ the hash function.

1. **Division Method:**
   $$h(k) = k \mod m$$
   - Pros: Simple and fast.
   - Cons: Choice of $m$ is critical. $m$ should be a prime number not close to powers of 2 or 10.

2. **Multiplication Method:**
   $$h(k) = \lfloor m \cdot (k \cdot A \mod 1) \rfloor$$
   - $A$ is a constant between 0 and 1 (e.g., $A \approx (\sqrt{5}-1)/2$).
   - Pros: Less sensitive to $m$.
   - Cons: More computationally intensive.

3. **Mid-Square Method:**
   - Square the key, extract middle digits as the hash value.
   - Pros: Good for some key sets.
   - Cons: Depends on key values, tricky to implement consistently.

4. **Folding Method:**
   - Divide the key into parts, combine using arithmetic (e.g., sum).
   - Pros: Handles large keys.
   - Cons: Choice of parts/operation affects distribution.

5. **Digit Analysis:**
   - Analyze digit distribution in keys, select most uniform digits.
   - Pros: Good for static key sets.
   - Cons: Requires prior knowledge of key distribution.

6. **String Keys:**
   - Convert string to integer (e.g., sum ASCII values, weighted sum with primes).
   - Weighted Summation: $h(s) = (s[0] \cdot p^0 + s[1] \cdot p^1 + \dots + s[L-1] \cdot p^{L-1}) \mod m$, where $p$ is a prime (e.g., 31, 37, 53).

---

## 3. Collision Resolution Techniques
Collisions are inevitable. When two keys map to the same index, we need a strategy to store both.

### 3.1. Separate Chaining (Open Hashing)
- Each slot in the hash table points to a linked list (or another structure).
- On collision, add the new key-value pair to the list at that index.
- **Advantages:**
  - Simple to implement.
  - Table never "fills up" (can store more elements than slots).
  - Performance degrades gracefully as load factor increases.
- **Disadvantages:**
  - Extra space for pointers.
  - Poor cache performance due to scattered memory.
  - Long lists degrade performance to $O(N)$.

### 3.2. Open Addressing (Closed Hashing)
- All elements are stored directly in the hash table array.
- On collision, "probe" for an alternative empty slot.
- **Load Factor ($\alpha$):** Number of elements / Table Size. For open addressing, $\alpha \leq 1$.
- **Deletion:** More complex; often uses a "deleted" marker.

#### 3.2.1. Linear Probing
- If $h(k)$ is occupied, try $(h(k) + 1) \mod m$, $(h(k) + 2) \mod m$, etc.
- **Advantages:** Good cache performance, no extra space for pointers.
- **Disadvantages:** Primary clustering (long runs of occupied slots).

#### 3.2.2. Quadratic Probing
- Next probe: $h(k, i) = (h(k) + c_1 i + c_2 i^2) \mod m$
- **Advantages:** Eliminates primary clustering.
- **Disadvantages:** Secondary clustering; only guaranteed to find an empty slot if $\alpha \leq 0.5$ and $m$ is prime.

#### 3.2.3. Double Hashing
- Uses two hash functions: $h_1(k)$ for initial probe, $h_2(k)$ for step size.
- $h(k, i) = (h_1(k) + i \cdot h_2(k)) \mod m$
- **Advantages:** Eliminates both primary and secondary clustering; excellent distribution.
- **Disadvantages:** More complex; requires careful hash function selection.

---

## 4. Performance Analysis
- **Load Factor ($\alpha$):**
  - Separate Chaining: $\alpha = N/M$ (can be $>1$)
  - Open Addressing: $\alpha = N/M$ ($\leq 1$)

### 4.1. Average Case Complexity (Uniform Hashing)
| Operation  | Separate Chaining | Open Addressing |
|------------|-------------------|-----------------|
| Search     | $O(1 + \alpha)$   | $O(1/(1-\alpha))$|
| Insertion  | $O(1 + \alpha)$   | $O(1/(1-\alpha))$|
| Deletion   | $O(1 + \alpha)$   | $O(1/(1-\alpha))$|

- When $\alpha$ is small (e.g., $\leq 0.7$), operations are effectively $O(1)$.
- For separate chaining, large $\alpha$ means long lists and degraded performance.
- For open addressing, as $\alpha$ approaches 1, performance degrades rapidly.

### 4.2. Worst Case Complexity
- All operations: $O(N)$ for both methods (all keys hash to the same slot or probe sequence).

---

## 5. Rehashing (Resizing)
When the load factor exceeds a threshold (e.g., 0.7 for open addressing, 1.0 for separate chaining), performance degrades. To maintain efficiency, the hash table is **rehashed**.

### 5.1. How Rehashing Works
1. Create a new, larger hash table (typically double the size, often a new prime number).
2. For each key-value pair in the old table, recompute its hash value using the new table size and insert into the new table.
3. Discard the old table.

### 5.2. Complexity
- Rehashing is $O(N)$, but happens infrequently. The amortized cost of insertion remains $O(1)$.

---

## 6. Advanced Hashing Techniques

### 6.1. Universal Hashing
- Uses a randomly chosen hash function from a family of functions at runtime.
- Guarantees good average-case performance regardless of input keys.
- Example: For prime $m$, $h_{a,b}(k) = ((a k + b) \mod p) \mod m$, where $p > m$ is prime, $a, b$ are random.

### 6.2. Perfect Hashing
- For static sets (keys known in advance), a perfect hash function maps all keys to distinct slots (zero collisions).
- Often uses a two-level scheme: primary hash to buckets, secondary collision-free hash for each bucket.
- $O(1)$ worst-case search time, but only for static sets.

### 6.3. Cuckoo Hashing
- Uses two (or more) hash functions. Each key has two (or more) possible locations.
- On insertion, if a slot is occupied, "kick out" the existing element and move it to its alternative location, repeating as needed.
- $O(1)$ worst-case search and deletion, $O(1)$ average-case insertion. Insertion may require rehashing.

### 6.4. Extendible Hashing
- Dynamic hashing for disk-based files; the hash table can grow or shrink.
- Uses a directory of pointers to buckets. Buckets split as needed, and the directory may double in size.
- Guarantees at most two disk accesses for search.

### 6.5. Cryptographic Hashing (Brief Distinction)
- **Data Structure Hashing:** Fast, uniform, minimizes collisions.
- **Cryptographic Hashing:** One-way, collision-resistant, avalanche effect. Used for data integrity, digital signatures, password storage.

---

## 7. Applications of Hashing
- **Databases:** Indexing, fast record retrieval.
- **Caches:** Storing frequently accessed data for quick lookup.
- **Symbol Tables:** In compilers/interpreters, mapping identifiers to properties.
- **Sets and Maps:** Implementing `HashSet`, `HashMap` in Java, `dict` in Python, `unordered_map` in C++.
- **Password Storage:** Storing hashes of passwords, not plain text.
- **Data Integrity:** Verifying file integrity (checksums).
- **Load Balancing:** Distributing requests across servers.
- **Spell Checkers:** Storing dictionary words for quick lookup.
- **Data Deduplication:** Identifying duplicate data blocks.

---

## 8. Conclusion
Hashing is a cornerstone of efficient data management. By understanding hash functions, collision resolution, and performance analysis, you can design data structures that offer near-constant time operations for a wide range of applications.

---
